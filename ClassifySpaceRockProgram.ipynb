{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI model to classify space rocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib. Use this library to plot your data. Add the following code in new cell in your Jupyter Notebook file, and then run the code.\n",
    "NumPy library to process large numerical matrixes (images), and run the new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch library to train and process deep learning and AI models. Torchvision, which is part of PyTorch. Use this library to process images and do manipulations like cropping and resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Imaging Library (PIL) to visualize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two libraries that ensure the plots are shown inline and with high resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Basalt', 'Highland']\n"
     ]
    }
   ],
   "source": [
    "# Tell the machine what folder contains the image data\n",
    "data_dir = './Data'\n",
    "\n",
    "# Read the data, crop and resize the images, split data into two groups: test and train\n",
    "def load_split_train_test(data_dir, valid_size = .2):\n",
    "\n",
    "    # Transform the images to train the model\n",
    "    train_transforms = transforms.Compose([\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.Resize(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ])\n",
    "\n",
    "    # Transform the images to test the model\n",
    "    test_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                          transforms.Resize(224),\n",
    "                                          transforms.ToTensor(),\n",
    "                                      ])\n",
    "\n",
    "    # Create two variables for the folders with the training and testing images\n",
    "    train_data = datasets.ImageFolder(data_dir, transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(data_dir, transform=test_transforms)\n",
    "\n",
    "    # Get the number of images in the training folder\n",
    "    num_train = len(train_data)\n",
    "\n",
    "    # Create a list of numbers from 0 to the number of training images - 1\n",
    "    # Example: For 10 images, the variable is the list [0,1,2,3,4,5,6,7,8,9]\n",
    "    indices = list(range(num_train))\n",
    "\n",
    "    # If valid_size is .2, find the index of the image that represents 20% of the data\n",
    "    # If there are 10 images, a split would result in 2\n",
    "    # split = int(np.floor(.2 * 10)) -> int(np.floor(2)) -> int(2) -> 2\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    # For 10 images, an example would be that indices is now the list [2,5,4,6,7,1,3,0,9,8]\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "    # With the indices randomly shuffled, \n",
    "    # grab the first 20% of the shuffled indices, and store them in the training index list\n",
    "    # grab the remainder of the shuffled indices, and store them in the testing index list\n",
    "    # Given our example so far, this would result is:\n",
    "    # train_idx is the list [2,5] \n",
    "    # test_idx is the list [4,6,7,1,3,0,9,8]\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # Create samplers to randomly grab items from the training and testing indices lists\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    # Create loaders to load 24 images from the train and test data folders\n",
    "    # Images are chosen based on the shuffled index lists and by using the samplers\n",
    "    trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=24)\n",
    "    testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=24)\n",
    "\n",
    "    # Return the loaders so you can grab images randomly from the training and testing data folders\n",
    "    return trainloader, testloader\n",
    "\n",
    "# Using the function that shuffles images,\n",
    "# create a trainloader to load 20% of the images\n",
    "# create a testloader to load 80% of the images\n",
    "trainloader, testloader = load_split_train_test(data_dir, .2)\n",
    "\n",
    "# Print the type of rocks that are included in the trainloader\n",
    "print(trainloader.dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect device type\n",
    "\n",
    "determine the most efficient way to create the deep learning network. First, find the type of device you're using: CPU or GPU. The PyTorch APIs offer support to form a neural network according to the device type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n",
    "# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n",
    "\n",
    "# Determine if you're using a CPU or a GPU device to build the deep learning network\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = models.resnet50(pretrained=True)\n",
    "\n",
    "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
    "weights = torchvision.models.ResNet50_Weights.DEFAULT # .DEFAULT = best available weights \n",
    "model = torchvision.models.resnet50(weights=weights).to(device)\n",
    "\n",
    "#model # uncomment to output (it's very long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build neurons and wire the network\n",
    "\n",
    "Build the neurons and wire the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build all the neurons\n",
    "for param in model.parameters():\n",
    "     param.requires_grad = False\n",
    "\n",
    "# Wire the neurons together to create the neural network\n",
    "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Dropout(0.2),\n",
    "                               nn.Linear(512, 2),\n",
    "                               nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "\n",
    "# Add the neural network to the device\n",
    "model.to(device)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network to accurately classify space rocks in photos\n",
    "## Iterate on the data and increase the accuracy\n",
    "In this section of code, look for the epochs variable. This variable tells the program how many times to search for associations in the features. In our example, we'll set the initial number of iterations to 10.\n",
    "\n",
    "To train our model, we load the image input from the trainloader variable that we built in the Analyze images of rocks with AI module. The data is stored to the already selected device. We call the optimizer.zero_grad() function to zero out gradients and avoid the accumulation of gradients across training iterations.\n",
    "\n",
    "The image input is passed through the model by using the model.forward(inputs) function, which returns the log probabilities of each label. The criterion(logps, labels) function runs the log probabilities through the criterion to get the output graph. The loss.backward() function uses the loss graph to compute the gradients. The optimizer.step() function then updates the parameters based on the current gradient.\n",
    "\n",
    "During the training and testing, we track the loss values for each iteration and the full batch. Every five epochs, we evaluate the model. We use the model.eval() function with the torch.no_grad() function to turn off parts of the model that behave differently during training versus evaluation. We use this pair of functions to refine the accuracy of the prediction without updating the gradients.\n",
    "\n",
    "The torch.exp(logps) function is used to get a new tensor with the true probabilities. The largest probability and class of the new tensor along a given dimension is returned from the ps.topk(1, dim=1) function. The tensor is reshaped to match the same shape as the top class.\n",
    "\n",
    "Finally, we compute the overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step  1\n",
      "Training step  2\n",
      "Training step  3\n",
      "Training step  4\n",
      "Training step  5\n",
      "Training step  6\n",
      "Training step  7\n",
      "Training step  8\n",
      "Training step  9\n",
      "Training step  10\n",
      "Training step  11\n",
      "Training step  12\n",
      "Training step  13\n",
      "Training step  14\n",
      "Training step  15\n",
      "Training step  16\n",
      "Training step  17\n",
      "Training step  18\n",
      "Training step  19\n",
      "Training step  20\n",
      "\n",
      "     Epoch 4/100: Train loss: 0.234.. Test loss: 0.287.. Test accuracy: 0.887\n",
      "\n",
      "Training step  21\n",
      "Training step  22\n",
      "Training step  23\n",
      "Training step  24\n",
      "Training step  25\n",
      "Training step  26\n",
      "Training step  27\n",
      "Training step  28\n",
      "Training step  29\n",
      "Training step  30\n",
      "Training step  31\n",
      "Training step  32\n",
      "Training step  33\n",
      "Training step  34\n",
      "Training step  35\n",
      "Training step  36\n",
      "Training step  37\n",
      "Training step  38\n",
      "Training step  39\n",
      "Training step  40\n",
      "\n",
      "     Epoch 8/100: Train loss: 0.100.. Test loss: 0.076.. Test accuracy: 0.979\n",
      "\n",
      "Training step  41\n",
      "Training step  42\n",
      "Training step  43\n",
      "Training step  44\n",
      "Training step  45\n",
      "Training step  46\n",
      "Training step  47\n",
      "Training step  48\n",
      "Training step  49\n",
      "Training step  50\n",
      "Training step  51\n",
      "Training step  52\n",
      "Training step  53\n",
      "Training step  54\n",
      "Training step  55\n",
      "Training step  56\n",
      "Training step  57\n",
      "Training step  58\n",
      "Training step  59\n",
      "Training step  60\n",
      "\n",
      "     Epoch 12/100: Train loss: 0.164.. Test loss: 0.445.. Test accuracy: 0.887\n",
      "\n",
      "Training step  61\n",
      "Training step  62\n",
      "Training step  63\n",
      "Training step  64\n",
      "Training step  65\n",
      "Training step  66\n",
      "Training step  67\n",
      "Training step  68\n",
      "Training step  69\n",
      "Training step  70\n",
      "Training step  71\n",
      "Training step  72\n",
      "Training step  73\n",
      "Training step  74\n",
      "Training step  75\n",
      "Training step  76\n",
      "Training step  77\n",
      "Training step  78\n",
      "Training step  79\n",
      "Training step  80\n",
      "\n",
      "     Epoch 16/100: Train loss: 0.424.. Test loss: 0.230.. Test accuracy: 0.896\n",
      "\n",
      "Training step  81\n",
      "Training step  82\n",
      "Training step  83\n",
      "Training step  84\n",
      "Training step  85\n",
      "Training step  86\n",
      "Training step  87\n",
      "Training step  88\n",
      "Training step  89\n",
      "Training step  90\n",
      "Training step  91\n",
      "Training step  92\n",
      "Training step  93\n",
      "Training step  94\n",
      "Training step  95\n",
      "Training step  96\n",
      "Training step  97\n",
      "Training step  98\n",
      "Training step  99\n",
      "Training step  100\n",
      "\n",
      "     Epoch 20/100: Train loss: 0.045.. Test loss: 0.092.. Test accuracy: 0.938\n",
      "\n",
      "Training step  101\n",
      "Training step  102\n",
      "Training step  103\n",
      "Training step  104\n",
      "Training step  105\n",
      "Training step  106\n",
      "Training step  107\n",
      "Training step  108\n",
      "Training step  109\n",
      "Training step  110\n",
      "Training step  111\n",
      "Training step  112\n",
      "Training step  113\n",
      "Training step  114\n",
      "Training step  115\n",
      "Training step  116\n",
      "Training step  117\n",
      "Training step  118\n",
      "Training step  119\n",
      "Training step  120\n",
      "\n",
      "     Epoch 24/100: Train loss: 0.026.. Test loss: 0.094.. Test accuracy: 0.979\n",
      "\n",
      "Training step  121\n",
      "Training step  122\n",
      "Training step  123\n",
      "Training step  124\n",
      "Training step  125\n",
      "Training step  126\n",
      "Training step  127\n",
      "Training step  128\n",
      "Training step  129\n",
      "Training step  130\n",
      "Training step  131\n",
      "Training step  132\n",
      "Training step  133\n",
      "Training step  134\n",
      "Training step  135\n",
      "Training step  136\n",
      "Training step  137\n",
      "Training step  138\n",
      "Training step  139\n",
      "Training step  140\n",
      "\n",
      "     Epoch 28/100: Train loss: 0.024.. Test loss: 0.144.. Test accuracy: 0.958\n",
      "\n",
      "Training step  141\n",
      "Training step  142\n",
      "Training step  143\n",
      "Training step  144\n",
      "Training step  145\n",
      "Training step  146\n",
      "Training step  147\n",
      "Training step  148\n",
      "Training step  149\n",
      "Training step  150\n",
      "Training step  151\n",
      "Training step  152\n",
      "Training step  153\n",
      "Training step  154\n",
      "Training step  155\n",
      "Training step  156\n",
      "Training step  157\n",
      "Training step  158\n",
      "Training step  159\n",
      "Training step  160\n",
      "\n",
      "     Epoch 32/100: Train loss: 0.018.. Test loss: 0.085.. Test accuracy: 0.958\n",
      "\n",
      "Training step  161\n",
      "Training step  162\n",
      "Training step  163\n",
      "Training step  164\n",
      "Training step  165\n",
      "Training step  166\n",
      "Training step  167\n",
      "Training step  168\n",
      "Training step  169\n",
      "Training step  170\n",
      "Training step  171\n",
      "Training step  172\n",
      "Training step  173\n",
      "Training step  174\n",
      "Training step  175\n",
      "Training step  176\n",
      "Training step  177\n",
      "Training step  178\n",
      "Training step  179\n",
      "Training step  180\n",
      "\n",
      "     Epoch 36/100: Train loss: 0.027.. Test loss: 0.104.. Test accuracy: 0.929\n",
      "\n",
      "Training step  181\n",
      "Training step  182\n",
      "Training step  183\n",
      "Training step  184\n",
      "Training step  185\n",
      "Training step  186\n",
      "Training step  187\n",
      "Training step  188\n",
      "Training step  189\n",
      "Training step  190\n",
      "Training step  191\n",
      "Training step  192\n",
      "Training step  193\n",
      "Training step  194\n",
      "Training step  195\n",
      "Training step  196\n",
      "Training step  197\n",
      "Training step  198\n",
      "Training step  199\n",
      "Training step  200\n",
      "\n",
      "     Epoch 40/100: Train loss: 0.028.. Test loss: 0.295.. Test accuracy: 0.908\n",
      "\n",
      "Training step  201\n",
      "Training step  202\n",
      "Training step  203\n",
      "Training step  204\n",
      "Training step  205\n",
      "Training step  206\n",
      "Training step  207\n",
      "Training step  208\n",
      "Training step  209\n",
      "Training step  210\n",
      "Training step  211\n",
      "Training step  212\n",
      "Training step  213\n",
      "Training step  214\n",
      "Training step  215\n",
      "Training step  216\n",
      "Training step  217\n",
      "Training step  218\n",
      "Training step  219\n",
      "Training step  220\n",
      "\n",
      "     Epoch 44/100: Train loss: 0.031.. Test loss: 0.060.. Test accuracy: 0.958\n",
      "\n",
      "Training step  221\n",
      "Training step  222\n",
      "Training step  223\n",
      "Training step  224\n",
      "Training step  225\n",
      "Training step  226\n",
      "Training step  227\n",
      "Training step  228\n",
      "Training step  229\n",
      "Training step  230\n",
      "Training step  231\n",
      "Training step  232\n",
      "Training step  233\n",
      "Training step  234\n",
      "Training step  235\n",
      "Training step  236\n",
      "Training step  237\n",
      "Training step  238\n",
      "Training step  239\n",
      "Training step  240\n",
      "\n",
      "     Epoch 48/100: Train loss: 0.031.. Test loss: 0.056.. Test accuracy: 0.979\n",
      "\n",
      "Training step  241\n",
      "Training step  242\n",
      "Training step  243\n",
      "Training step  244\n",
      "Training step  245\n",
      "Training step  246\n",
      "Training step  247\n",
      "Training step  248\n",
      "Training step  249\n",
      "Training step  250\n",
      "Training step  251\n",
      "Training step  252\n",
      "Training step  253\n",
      "Training step  254\n",
      "Training step  255\n",
      "Training step  256\n",
      "Training step  257\n",
      "Training step  258\n",
      "Training step  259\n",
      "Training step  260\n",
      "\n",
      "     Epoch 52/100: Train loss: 0.031.. Test loss: 0.042.. Test accuracy: 0.979\n",
      "\n",
      "Training step  261\n",
      "Training step  262\n",
      "Training step  263\n",
      "Training step  264\n",
      "Training step  265\n",
      "Training step  266\n",
      "Training step  267\n",
      "Training step  268\n",
      "Training step  269\n",
      "Training step  270\n",
      "Training step  271\n",
      "Training step  272\n",
      "Training step  273\n",
      "Training step  274\n",
      "Training step  275\n",
      "Training step  276\n",
      "Training step  277\n",
      "Training step  278\n",
      "Training step  279\n",
      "Training step  280\n",
      "\n",
      "     Epoch 56/100: Train loss: 0.022.. Test loss: 0.188.. Test accuracy: 0.929\n",
      "\n",
      "Training step  281\n",
      "Training step  282\n",
      "Training step  283\n",
      "Training step  284\n",
      "Training step  285\n",
      "Training step  286\n",
      "Training step  287\n",
      "Training step  288\n",
      "Training step  289\n",
      "Training step  290\n",
      "Training step  291\n",
      "Training step  292\n",
      "Training step  293\n",
      "Training step  294\n",
      "Training step  295\n",
      "Training step  296\n",
      "Training step  297\n",
      "Training step  298\n",
      "Training step  299\n",
      "Training step  300\n",
      "\n",
      "     Epoch 60/100: Train loss: 0.021.. Test loss: 0.014.. Test accuracy: 1.000\n",
      "\n",
      "Training step  301\n",
      "Training step  302\n",
      "Training step  303\n",
      "Training step  304\n",
      "Training step  305\n",
      "Training step  306\n",
      "Training step  307\n",
      "Training step  308\n",
      "Training step  309\n",
      "Training step  310\n",
      "Training step  311\n",
      "Training step  312\n",
      "Training step  313\n",
      "Training step  314\n",
      "Training step  315\n",
      "Training step  316\n",
      "Training step  317\n",
      "Training step  318\n",
      "Training step  319\n",
      "Training step  320\n",
      "\n",
      "     Epoch 64/100: Train loss: 0.029.. Test loss: 0.010.. Test accuracy: 1.000\n",
      "\n",
      "Training step  321\n",
      "Training step  322\n",
      "Training step  323\n",
      "Training step  324\n",
      "Training step  325\n",
      "Training step  326\n",
      "Training step  327\n",
      "Training step  328\n",
      "Training step  329\n",
      "Training step  330\n",
      "Training step  331\n",
      "Training step  332\n",
      "Training step  333\n",
      "Training step  334\n",
      "Training step  335\n",
      "Training step  336\n",
      "Training step  337\n",
      "Training step  338\n",
      "Training step  339\n",
      "Training step  340\n",
      "\n",
      "     Epoch 68/100: Train loss: 0.006.. Test loss: 0.008.. Test accuracy: 1.000\n",
      "\n",
      "Training step  341\n",
      "Training step  342\n",
      "Training step  343\n",
      "Training step  344\n",
      "Training step  345\n",
      "Training step  346\n",
      "Training step  347\n",
      "Training step  348\n",
      "Training step  349\n",
      "Training step  350\n",
      "Training step  351\n",
      "Training step  352\n",
      "Training step  353\n",
      "Training step  354\n",
      "Training step  355\n",
      "Training step  356\n",
      "Training step  357\n",
      "Training step  358\n",
      "Training step  359\n",
      "Training step  360\n",
      "\n",
      "     Epoch 72/100: Train loss: 0.010.. Test loss: 0.018.. Test accuracy: 1.000\n",
      "\n",
      "Training step  361\n",
      "Training step  362\n",
      "Training step  363\n",
      "Training step  364\n",
      "Training step  365\n",
      "Training step  366\n",
      "Training step  367\n",
      "Training step  368\n",
      "Training step  369\n",
      "Training step  370\n",
      "Training step  371\n",
      "Training step  372\n",
      "Training step  373\n",
      "Training step  374\n",
      "Training step  375\n",
      "Training step  376\n",
      "Training step  377\n",
      "Training step  378\n",
      "Training step  379\n",
      "Training step  380\n",
      "\n",
      "     Epoch 76/100: Train loss: 0.004.. Test loss: 0.012.. Test accuracy: 1.000\n",
      "\n",
      "Training step  381\n",
      "Training step  382\n",
      "Training step  383\n",
      "Training step  384\n",
      "Training step  385\n",
      "Training step  386\n",
      "Training step  387\n",
      "Training step  388\n",
      "Training step  389\n",
      "Training step  390\n",
      "Training step  391\n",
      "Training step  392\n",
      "Training step  393\n",
      "Training step  394\n",
      "Training step  395\n",
      "Training step  396\n",
      "Training step  397\n",
      "Training step  398\n",
      "Training step  399\n",
      "Training step  400\n",
      "\n",
      "     Epoch 80/100: Train loss: 0.044.. Test loss: 0.026.. Test accuracy: 0.979\n",
      "\n",
      "Training step  401\n",
      "Training step  402\n",
      "Training step  403\n",
      "Training step  404\n",
      "Training step  405\n",
      "Training step  406\n",
      "Training step  407\n",
      "Training step  408\n",
      "Training step  409\n",
      "Training step  410\n",
      "Training step  411\n",
      "Training step  412\n",
      "Training step  413\n",
      "Training step  414\n",
      "Training step  415\n",
      "Training step  416\n",
      "Training step  417\n",
      "Training step  418\n",
      "Training step  419\n",
      "Training step  420\n",
      "\n",
      "     Epoch 84/100: Train loss: 0.024.. Test loss: 0.358.. Test accuracy: 0.929\n",
      "\n",
      "Training step  421\n",
      "Training step  422\n",
      "Training step  423\n",
      "Training step  424\n",
      "Training step  425\n",
      "Training step  426\n",
      "Training step  427\n",
      "Training step  428\n",
      "Training step  429\n",
      "Training step  430\n",
      "Training step  431\n",
      "Training step  432\n",
      "Training step  433\n",
      "Training step  434\n",
      "Training step  435\n",
      "Training step  436\n",
      "Training step  437\n",
      "Training step  438\n",
      "Training step  439\n",
      "Training step  440\n",
      "\n",
      "     Epoch 88/100: Train loss: 0.022.. Test loss: 0.293.. Test accuracy: 0.958\n",
      "\n",
      "Training step  441\n",
      "Training step  442\n",
      "Training step  443\n",
      "Training step  444\n",
      "Training step  445\n",
      "Training step  446\n",
      "Training step  447\n",
      "Training step  448\n",
      "Training step  449\n",
      "Training step  450\n",
      "Training step  451\n",
      "Training step  452\n",
      "Training step  453\n",
      "Training step  454\n",
      "Training step  455\n",
      "Training step  456\n",
      "Training step  457\n",
      "Training step  458\n",
      "Training step  459\n",
      "Training step  460\n",
      "\n",
      "     Epoch 92/100: Train loss: 0.050.. Test loss: 0.136.. Test accuracy: 0.979\n",
      "\n",
      "Training step  461\n",
      "Training step  462\n",
      "Training step  463\n",
      "Training step  464\n",
      "Training step  465\n",
      "Training step  466\n",
      "Training step  467\n",
      "Training step  468\n",
      "Training step  469\n",
      "Training step  470\n",
      "Training step  471\n",
      "Training step  472\n",
      "Training step  473\n",
      "Training step  474\n",
      "Training step  475\n",
      "Training step  476\n",
      "Training step  477\n",
      "Training step  478\n",
      "Training step  479\n",
      "Training step  480\n",
      "\n",
      "     Epoch 96/100: Train loss: 0.027.. Test loss: 0.158.. Test accuracy: 0.979\n",
      "\n",
      "Training step  481\n",
      "Training step  482\n",
      "Training step  483\n",
      "Training step  484\n",
      "Training step  485\n",
      "Training step  486\n",
      "Training step  487\n",
      "Training step  488\n",
      "Training step  489\n",
      "Training step  490\n",
      "Training step  491\n",
      "Training step  492\n",
      "Training step  493\n",
      "Training step  494\n",
      "Training step  495\n",
      "Training step  496\n",
      "Training step  497\n",
      "Training step  498\n",
      "Training step  499\n",
      "Training step  500\n",
      "\n",
      "     Epoch 100/100: Train loss: 0.119.. Test loss: 0.120.. Test accuracy: 0.938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the initial number of iterations to search for associations\n",
    "epochs = 100\n",
    "print_every = 20\n",
    "\n",
    "# Initialize the loss variables\n",
    "running_loss = 0\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "# Track the current training step, start at 0\n",
    "steps = 0\n",
    "\n",
    "# Search for associations in the features\n",
    "for epoch in range(epochs):\n",
    "\n",
    "   # Count each epoch\n",
    "   epoch += 1\n",
    "\n",
    "   # Load in all of the image inputs and labels from the TRAIN loader \n",
    "   for inputs, labels in trainloader:\n",
    "\n",
    "      # Count each training step\n",
    "      steps += 1\n",
    "      print('Training step ', steps)\n",
    "\n",
    "      # Load the inputs and labels to the already selected device\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      # Zero out gradients to avoid accumulations of gradiants across training iterations\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Pass the images through the model, return the log probabilities of each label\n",
    "      logps = model.forward(inputs)\n",
    "\n",
    "      # Run the log probabilities through the criterion to get the output graph\n",
    "      loss = criterion(logps, labels)\n",
    "\n",
    "      # Use the loss graph to compute gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # Update the parameters based on the current gradient\n",
    "      optimizer.step()\n",
    "\n",
    "      # Add the actual loss number to the running loss total\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # Every 5 steps, evaluate the model\n",
    "      if steps % print_every == 0:\n",
    "\n",
    "         # Initialize loss and accuracy\n",
    "         test_loss = 0\n",
    "         accuracy = 0\n",
    "\n",
    "         # Start the model evaluation\n",
    "         model.eval()\n",
    "\n",
    "         # Refine the accuracy of the prediction without updating the gradients\n",
    "         with torch.no_grad():\n",
    "\n",
    "            # Load in all of the image inputs and labels from the TEST loader \n",
    "            for inputs, labels in testloader:\n",
    "\n",
    "               # Load the inputs and labels to the already selected device\n",
    "               inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "               # Pass the images through the model, return the log probabilities of each label\n",
    "               logps = model.forward(inputs)\n",
    "\n",
    "               # Run the log probabilities through the criterion to get the output graph\n",
    "               batch_loss = criterion(logps, labels)\n",
    "\n",
    "               # Add the actual loss number to the running loss total for the test batch\n",
    "               test_loss += batch_loss.item()\n",
    "\n",
    "               # Return a new tensor with the true probabilities\n",
    "               ps = torch.exp(logps)\n",
    "\n",
    "               # Return the largest probability and class of the new tensor along a given dimension\n",
    "               top_p, top_class = ps.topk(1, dim=1)\n",
    "\n",
    "               # Reshape the tensor to match the same shape as the top class\n",
    "               equals = top_class == labels.view(*top_class.shape)\n",
    "\n",
    "               # Compute the accuracy and add it to the running accuracy count for the test batch\n",
    "               accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "         # Append the training and testing losses\n",
    "         train_losses.append(running_loss/len(trainloader))\n",
    "         test_losses.append(test_loss/len(testloader))  \n",
    "\n",
    "         # Display the accuracy of the prediction with 3 digits in the fractional part of the decimal\n",
    "         print(f\"\\n     Epoch {epoch}/{epochs}: \"\n",
    "               f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "               f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "               f\"Test accuracy: {accuracy/len(testloader):.3f}\\n\")\n",
    "\n",
    "         # Train the model\n",
    "         running_loss = 0\n",
    "         model.train()\n",
    "\n",
    "         # After 10 training steps, start the next epoch\n",
    "         # Break here in case the trainloader has remaining data\n",
    "         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the training output\n",
    "After five epochs are complete, the system reaches our epoch limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the prediction accuracy for each epoch iteration with training and testing losses, and the test accuracy.\n",
    "\n",
    "Here are the results from our test with five epochs. Your specific results will differ because the computer chooses a set of random images for each test run. The results reveal the training loss, testing loss, and accuracy, all depend on the chosen image.\n",
    "\n",
    "| Epoch\t| Training loss\t| Test loss\t| Test accuracy |\n",
    "| ----- | ------------- | --------- | ------------- |\n",
    "| 1\t    | 0.550\t        | 0.282\t    | 0.902         |\n",
    "| 2\t    | 0.451\t        | 0.311\t    | 0.842         |\n",
    "| 3\t    | 0.342\t        | 0.233\t    | 0.902         |\n",
    "| 4\t    | 0.216\t        | 0.189\t    | 0.906         |\n",
    "| 5\t    | 0.234\t        | 0.175\t    | 0.935         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n"
     ]
    }
   ],
   "source": [
    "# The following code calculates and displays the accuracy of our AI model to classify the rock type.\n",
    "print(accuracy/len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model...\n",
    "torch.save(model, 'aerialmodel.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model to make predictions\n",
    "\n",
    "Steps to make predictions with the neural network in your AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load the neural network\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=torch.load('aerialmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to predict the type of rock in a new image by comparing it against our model's matrix pattern\n",
    "def predict_image(image):\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    index = output.data.cpu().numpy().argmax()\n",
    "    return index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
