{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI model to classify space rocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib. Use this library to plot your data. Add the following code in new cell in your Jupyter Notebook file, and then run the code.\n",
    "NumPy library to process large numerical matrixes (images), and run the new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch library to train and process deep learning and AI models. Torchvision, which is part of PyTorch. Use this library to process images and do manipulations like cropping and resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Imaging Library (PIL) to visualize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two libraries that ensure the plots are shown inline and with high resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Basalt', 'Highland']\n"
     ]
    }
   ],
   "source": [
    "# Tell the machine what folder contains the image data\n",
    "data_dir = './Data'\n",
    "\n",
    "# Read the data, crop and resize the images, split data into two groups: test and train\n",
    "def load_split_train_test(data_dir, valid_size = .2):\n",
    "\n",
    "    # Transform the images to train the model\n",
    "    train_transforms = transforms.Compose([\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.Resize(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ])\n",
    "\n",
    "    # Transform the images to test the model\n",
    "    test_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                          transforms.Resize(224),\n",
    "                                          transforms.ToTensor(),\n",
    "                                      ])\n",
    "\n",
    "    # Create two variables for the folders with the training and testing images\n",
    "    train_data = datasets.ImageFolder(data_dir, transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(data_dir, transform=test_transforms)\n",
    "\n",
    "    # Get the number of images in the training folder\n",
    "    num_train = len(train_data)\n",
    "\n",
    "    # Create a list of numbers from 0 to the number of training images - 1\n",
    "    # Example: For 10 images, the variable is the list [0,1,2,3,4,5,6,7,8,9]\n",
    "    indices = list(range(num_train))\n",
    "\n",
    "    # If valid_size is .2, find the index of the image that represents 20% of the data\n",
    "    # If there are 10 images, a split would result in 2\n",
    "    # split = int(np.floor(.2 * 10)) -> int(np.floor(2)) -> int(2) -> 2\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    # For 10 images, an example would be that indices is now the list [2,5,4,6,7,1,3,0,9,8]\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "    # With the indices randomly shuffled, \n",
    "    # grab the first 20% of the shuffled indices, and store them in the training index list\n",
    "    # grab the remainder of the shuffled indices, and store them in the testing index list\n",
    "    # Given our example so far, this would result is:\n",
    "    # train_idx is the list [1,5] \n",
    "    # test_idx is the list [4,6,7,1,3,0,9,8]\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # Create samplers to randomly grab items from the training and testing indices lists\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    # Create loaders to load 16 images from the train and test data folders\n",
    "    # Images are chosen based on the shuffled index lists and by using the samplers\n",
    "    trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
    "    testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=16)\n",
    "\n",
    "    # Return the loaders so you can grab images randomly from the training and testing data folders\n",
    "    return trainloader, testloader\n",
    "\n",
    "# Using the function that shuffles images,\n",
    "# create a trainloader to load 20% of the images\n",
    "# create a testloader to load 80% of the images\n",
    "trainloader, testloader = load_split_train_test(data_dir, .2)\n",
    "\n",
    "# Print the type of rocks that are included in the trainloader\n",
    "print(trainloader.dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect device type\n",
    "\n",
    "determine the most efficient way to create the deep learning network. First, find the type of device you're using: CPU or GPU. The PyTorch APIs offer support to form a neural network according to the device type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n",
    "# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n",
    "\n",
    "# Determine if you're using a CPU or a GPU device to build the deep learning network\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = models.resnet50(pretrained=True)\n",
    "\n",
    "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
    "weights = torchvision.models.ResNet50_Weights.DEFAULT # .DEFAULT = best available weights \n",
    "model = torchvision.models.resnet50(weights=weights).to(device)\n",
    "\n",
    "#model # uncomment to output (it's very long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build neurons and wire the network\n",
    "\n",
    "Build the neurons and wire the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build all the neurons\n",
    "for param in model.parameters():\n",
    "     param.requires_grad = False\n",
    "\n",
    "# Wire the neurons together to create the neural network\n",
    "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Dropout(0.2),\n",
    "                               nn.Linear(512, 2),\n",
    "                               nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "\n",
    "# Add the neural network to the device\n",
    "model.to(device)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network to accurately classify space rocks in photos\n",
    "## Iterate on the data and increase the accuracy\n",
    "In this section of code, look for the epochs variable. This variable tells the program how many times to search for associations in the features. In our example, we'll set the initial number of iterations to 10.\n",
    "\n",
    "To train our model, we load the image input from the trainloader variable that we built in the Analyze images of rocks with AI module. The data is stored to the already selected device. We call the optimizer.zero_grad() function to zero out gradients and avoid the accumulation of gradients across training iterations.\n",
    "\n",
    "The image input is passed through the model by using the model.forward(inputs) function, which returns the log probabilities of each label. The criterion(logps, labels) function runs the log probabilities through the criterion to get the output graph. The loss.backward() function uses the loss graph to compute the gradients. The optimizer.step() function then updates the parameters based on the current gradient.\n",
    "\n",
    "During the training and testing, we track the loss values for each iteration and the full batch. Every five epochs, we evaluate the model. We use the model.eval() function with the torch.no_grad() function to turn off parts of the model that behave differently during training versus evaluation. We use this pair of functions to refine the accuracy of the prediction without updating the gradients.\n",
    "\n",
    "The torch.exp(logps) function is used to get a new tensor with the true probabilities. The largest probability and class of the new tensor along a given dimension is returned from the ps.topk(1, dim=1) function. The tensor is reshaped to match the same shape as the top class.\n",
    "\n",
    "Finally, we compute the overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step  1\n",
      "Training step  2\n",
      "Training step  3\n",
      "Training step  4\n",
      "Training step  5\n",
      "Training step  6\n",
      "Training step  7\n",
      "Training step  8\n",
      "Training step  9\n",
      "Training step  10\n",
      "Training step  11\n",
      "Training step  12\n",
      "Training step  13\n",
      "Training step  14\n",
      "Training step  15\n",
      "\n",
      "     Epoch 2/15: Train loss: 0.060.. Test loss: 0.141.. Test accuracy: 0.969\n",
      "\n",
      "Training step  16\n",
      "Training step  17\n",
      "Training step  18\n",
      "Training step  19\n",
      "Training step  20\n",
      "Training step  21\n",
      "Training step  22\n",
      "Training step  23\n",
      "Training step  24\n",
      "Training step  25\n",
      "Training step  26\n",
      "Training step  27\n",
      "Training step  28\n",
      "Training step  29\n",
      "Training step  30\n",
      "\n",
      "     Epoch 4/15: Train loss: 0.032.. Test loss: 0.259.. Test accuracy: 0.869\n",
      "\n",
      "Training step  31\n",
      "Training step  32\n",
      "Training step  33\n",
      "Training step  34\n",
      "Training step  35\n",
      "Training step  36\n",
      "Training step  37\n",
      "Training step  38\n",
      "Training step  39\n",
      "Training step  40\n",
      "Training step  41\n",
      "Training step  42\n",
      "Training step  43\n",
      "Training step  44\n",
      "Training step  45\n",
      "\n",
      "     Epoch 6/15: Train loss: 0.086.. Test loss: 0.077.. Test accuracy: 0.967\n",
      "\n",
      "Training step  46\n",
      "Training step  47\n",
      "Training step  48\n",
      "Training step  49\n",
      "Training step  50\n",
      "Training step  51\n",
      "Training step  52\n",
      "Training step  53\n",
      "Training step  54\n",
      "Training step  55\n",
      "Training step  56\n",
      "Training step  57\n",
      "Training step  58\n",
      "Training step  59\n",
      "Training step  60\n",
      "\n",
      "     Epoch 8/15: Train loss: 0.138.. Test loss: 0.243.. Test accuracy: 0.873\n",
      "\n",
      "Training step  61\n",
      "Training step  62\n",
      "Training step  63\n",
      "Training step  64\n",
      "Training step  65\n",
      "Training step  66\n",
      "Training step  67\n",
      "Training step  68\n",
      "Training step  69\n",
      "Training step  70\n",
      "Training step  71\n",
      "Training step  72\n",
      "Training step  73\n",
      "Training step  74\n",
      "Training step  75\n",
      "\n",
      "     Epoch 10/15: Train loss: 0.048.. Test loss: 0.385.. Test accuracy: 0.904\n",
      "\n",
      "Training step  76\n",
      "Training step  77\n",
      "Training step  78\n",
      "Training step  79\n",
      "Training step  80\n",
      "Training step  81\n",
      "Training step  82\n",
      "Training step  83\n",
      "Training step  84\n",
      "Training step  85\n",
      "Training step  86\n",
      "Training step  87\n",
      "Training step  88\n",
      "Training step  89\n",
      "Training step  90\n",
      "\n",
      "     Epoch 12/15: Train loss: 0.042.. Test loss: 0.072.. Test accuracy: 0.967\n",
      "\n",
      "Training step  91\n",
      "Training step  92\n",
      "Training step  93\n",
      "Training step  94\n",
      "Training step  95\n",
      "Training step  96\n",
      "Training step  97\n",
      "Training step  98\n",
      "Training step  99\n",
      "Training step  100\n",
      "Training step  101\n",
      "Training step  102\n",
      "Training step  103\n",
      "Training step  104\n",
      "Training step  105\n",
      "\n",
      "     Epoch 14/15: Train loss: 0.044.. Test loss: 0.107.. Test accuracy: 0.935\n",
      "\n",
      "Training step  106\n",
      "Training step  107\n",
      "Training step  108\n",
      "Training step  109\n",
      "Training step  110\n",
      "Training step  111\n",
      "Training step  112\n",
      "Training step  113\n"
     ]
    }
   ],
   "source": [
    "# Set the initial number of iterations to search for associations\n",
    "epochs = 20\n",
    "print_every = 40\n",
    "\n",
    "# Initialize the loss variables\n",
    "running_loss = 0\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "# Track the current training step, start at 0\n",
    "steps = 0\n",
    "\n",
    "# Search for associations in the features\n",
    "for epoch in range(epochs):\n",
    "\n",
    "   # Count each epoch\n",
    "   epoch += 1\n",
    "\n",
    "   # Load in all of the image inputs and labels from the TRAIN loader \n",
    "   for inputs, labels in trainloader:\n",
    "\n",
    "      # Count each training step\n",
    "      steps += 1\n",
    "      print('Training step ', steps)\n",
    "\n",
    "      # Load the inputs and labels to the already selected device\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      # Zero out gradients to avoid accumulations of gradiants across training iterations\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Pass the images through the model, return the log probabilities of each label\n",
    "      logps = model.forward(inputs)\n",
    "\n",
    "      # Run the log probabilities through the criterion to get the output graph\n",
    "      loss = criterion(logps, labels)\n",
    "\n",
    "      # Use the loss graph to compute gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # Update the parameters based on the current gradient\n",
    "      optimizer.step()\n",
    "\n",
    "      # Add the actual loss number to the running loss total\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # Every 5 steps, evaluate the model\n",
    "      if steps % print_every == 0:\n",
    "\n",
    "         # Initialize loss and accuracy\n",
    "         test_loss = 0\n",
    "         accuracy = 0\n",
    "\n",
    "         # Start the model evaluation\n",
    "         model.eval()\n",
    "\n",
    "         # Refine the accuracy of the prediction without updating the gradients\n",
    "         with torch.no_grad():\n",
    "\n",
    "            # Load in all of the image inputs and labels from the TEST loader \n",
    "            for inputs, labels in testloader:\n",
    "\n",
    "               # Load the inputs and labels to the already selected device\n",
    "               inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "               # Pass the images through the model, return the log probabilities of each label\n",
    "               logps = model.forward(inputs)\n",
    "\n",
    "               # Run the log probabilities through the criterion to get the output graph\n",
    "               batch_loss = criterion(logps, labels)\n",
    "\n",
    "               # Add the actual loss number to the running loss total for the test batch\n",
    "               test_loss += batch_loss.item()\n",
    "\n",
    "               # Return a new tensor with the true probabilities\n",
    "               ps = torch.exp(logps)\n",
    "\n",
    "               # Return the largest probability and class of the new tensor along a given dimension\n",
    "               top_p, top_class = ps.topk(1, dim=1)\n",
    "\n",
    "               # Reshape the tensor to match the same shape as the top class\n",
    "               equals = top_class == labels.view(*top_class.shape)\n",
    "\n",
    "               # Compute the accuracy and add it to the running accuracy count for the test batch\n",
    "               accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "         # Append the training and testing losses\n",
    "         train_losses.append(running_loss/len(trainloader))\n",
    "         test_losses.append(test_loss/len(testloader))  \n",
    "\n",
    "         # Display the accuracy of the prediction with 3 digits in the fractional part of the decimal\n",
    "         print(f\"\\n     Epoch {epoch}/{epochs}: \"\n",
    "               f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "               f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "               f\"Test accuracy: {accuracy/len(testloader):.3f}\\n\")\n",
    "\n",
    "         # Train the model\n",
    "         running_loss = 0\n",
    "         model.train()\n",
    "\n",
    "         # After 10 training steps, start the next epoch\n",
    "         # Break here in case the trainloader has remaining data\n",
    "         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the training output\n",
    "After five epochs are complete, the system reaches our epoch limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the prediction accuracy for each epoch iteration with training and testing losses, and the test accuracy.\n",
    "\n",
    "Here are the results from our test with five epochs. Your specific results will differ because the computer chooses a set of random images for each test run. The results reveal the training loss, testing loss, and accuracy, all depend on the chosen image.\n",
    "\n",
    "| Epoch\t| Training loss\t| Test loss\t| Test accuracy |\n",
    "| ----- | ------------- | --------- | ------------- |\n",
    "| 1\t    | 0.550\t        | 0.282\t    | 0.902         |\n",
    "| 2\t    | 0.451\t        | 0.311\t    | 0.842         |\n",
    "| 3\t    | 0.342\t        | 0.233\t    | 0.902         |\n",
    "| 4\t    | 0.216\t        | 0.189\t    | 0.906         |\n",
    "| 5\t    | 0.234\t        | 0.175\t    | 0.935         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9354166686534882\n"
     ]
    }
   ],
   "source": [
    "# The following code calculates and displays the accuracy of our AI model to classify the rock type.\n",
    "print(accuracy/len(testloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
