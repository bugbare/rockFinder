{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI model to classify space rocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib. Use this library to plot your data. Add the following code in new cell in your Jupyter Notebook file, and then run the code.\n",
    "NumPy library to process large numerical matrixes (images), and run the new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch library to train and process deep learning and AI models. Torchvision, which is part of PyTorch. Use this library to process images and do manipulations like cropping and resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Imaging Library (PIL) to visualize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two libraries that ensure the plots are shown inline and with high resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell the machine what folder contains the image data\n",
    "data_dir = './Data'\n",
    "\n",
    "# Read the data, crop and resize the images, split data into two groups: test and train\n",
    "def load_split_train_test(data_dir, valid_size = .2):\n",
    "\n",
    "    # Transform the images to train the model\n",
    "    train_transforms = transforms.Compose([\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.Resize(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ])\n",
    "\n",
    "    # Transform the images to test the model\n",
    "    test_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                          transforms.Resize(224),\n",
    "                                          transforms.ToTensor(),\n",
    "                                      ])\n",
    "\n",
    "    # Create two variables for the folders with the training and testing images\n",
    "    train_data = datasets.ImageFolder(data_dir, transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(data_dir, transform=test_transforms)\n",
    "\n",
    "    # Get the number of images in the training folder\n",
    "    num_train = len(train_data)\n",
    "\n",
    "    # Create a list of numbers from 0 to the number of training images - 1\n",
    "    # Example: For 10 images, the variable is the list [0,1,2,3,4,5,6,7,8,9]\n",
    "    indices = list(range(num_train))\n",
    "\n",
    "    # If valid_size is .2, find the index of the image that represents 20% of the data\n",
    "    # If there are 10 images, a split would result in 2\n",
    "    # split = int(np.floor(.2 * 10)) -> int(np.floor(2)) -> int(2) -> 2\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    # For 10 images, an example would be that indices is now the list [2,5,4,6,7,1,3,0,9,8]\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # With the indices randomly shuffled, \n",
    "    # grab the first 20% of the shuffled indices, and store them in the training index list\n",
    "    # grab the remainder of the shuffled indices, and store them in the testing index list\n",
    "    # Given our example so far, this would result is:\n",
    "    # train_idx is the list [2,5] \n",
    "    # test_idx is the list [4,6,7,1,3,0,9,8]\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # Create samplers to randomly grab items from the training and testing indices lists\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    # Create loaders to load 24 images from the train and test data folders\n",
    "    # Images are chosen based on the shuffled index lists and by using the samplers\n",
    "    trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=24)\n",
    "    testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=24)\n",
    "\n",
    "    # Return the loaders so you can grab images randomly from the training and testing data folders\n",
    "    return trainloader, testloader\n",
    "\n",
    "# Using the function that shuffles images,\n",
    "# create a trainloader to load 20% of the images\n",
    "# create a testloader to load 80% of the images\n",
    "trainloader, testloader = load_split_train_test(data_dir, .2)\n",
    "\n",
    "# Print the type of rocks that are included in the trainloader\n",
    "print(trainloader.dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect device type\n",
    "\n",
    "determine the most efficient way to create the deep learning network. First, find the type of device you're using: CPU or GPU. The PyTorch APIs offer support to form a neural network according to the device type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n",
    "# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n",
    "\n",
    "# Determine if you're using a CPU or a GPU device to build the deep learning network\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = models.resnet50(pretrained=True)\n",
    "\n",
    "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
    "weights = torchvision.models.ResNet50_Weights.DEFAULT # .DEFAULT = best available weights \n",
    "model = torchvision.models.resnet50(weights=weights).to(device)\n",
    "\n",
    "#model # uncomment to output (it's very long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build neurons and wire the network\n",
    "\n",
    "Build the neurons and wire the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all the neurons\n",
    "for param in model.parameters():\n",
    "     param.requires_grad = False\n",
    "\n",
    "# Wire the neurons together to create the neural network\n",
    "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Dropout(0.2),\n",
    "                               nn.Linear(512, 2),\n",
    "                               nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "\n",
    "# Add the neural network to the device\n",
    "model.to(device)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network to accurately classify space rocks in photos\n",
    "## Iterate on the data and increase the accuracy\n",
    "In this section of code, look for the epochs variable. This variable tells the program how many times to search for associations in the features. In our example, we'll set the initial number of iterations to 10.\n",
    "\n",
    "To train our model, we load the image input from the trainloader variable that we built in the Analyze images of rocks with AI module. The data is stored to the already selected device. We call the optimizer.zero_grad() function to zero out gradients and avoid the accumulation of gradients across training iterations.\n",
    "\n",
    "The image input is passed through the model by using the model.forward(inputs) function, which returns the log probabilities of each label. The criterion(logps, labels) function runs the log probabilities through the criterion to get the output graph. The loss.backward() function uses the loss graph to compute the gradients. The optimizer.step() function then updates the parameters based on the current gradient.\n",
    "\n",
    "During the training and testing, we track the loss values for each iteration and the full batch. Every five epochs, we evaluate the model. We use the model.eval() function with the torch.no_grad() function to turn off parts of the model that behave differently during training versus evaluation. We use this pair of functions to refine the accuracy of the prediction without updating the gradients.\n",
    "\n",
    "The torch.exp(logps) function is used to get a new tensor with the true probabilities. The largest probability and class of the new tensor along a given dimension is returned from the ps.topk(1, dim=1) function. The tensor is reshaped to match the same shape as the top class.\n",
    "\n",
    "Finally, we compute the overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial number of iterations to search for associations\n",
    "epochs = 15\n",
    "print_every = 3\n",
    "\n",
    "# Initialize the loss variables\n",
    "running_loss = 0\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "# Track the current training step, start at 0\n",
    "steps = 0\n",
    "\n",
    "# Search for associations in the features\n",
    "for epoch in range(epochs):\n",
    "\n",
    "   # Count each epoch\n",
    "   epoch += 1\n",
    "\n",
    "   # Load in all of the image inputs and labels from the TRAIN loader \n",
    "   for inputs, labels in trainloader:\n",
    "\n",
    "      # Count each training step\n",
    "      steps += 1\n",
    "      print('Training step ', steps)\n",
    "\n",
    "      # Load the inputs and labels to the already selected device\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      # Zero out gradients to avoid accumulations of gradiants across training iterations\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Pass the images through the model, return the log probabilities of each label\n",
    "      logps = model.forward(inputs)\n",
    "\n",
    "      # Run the log probabilities through the criterion to get the output graph\n",
    "      loss = criterion(logps, labels)\n",
    "\n",
    "      # Use the loss graph to compute gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # Update the parameters based on the current gradient\n",
    "      optimizer.step()\n",
    "\n",
    "      # Add the actual loss number to the running loss total\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # Every 5 steps, evaluate the model\n",
    "      if steps % print_every == 0:\n",
    "\n",
    "         # Initialize loss and accuracy\n",
    "         test_loss = 0\n",
    "         accuracy = 0\n",
    "\n",
    "         # Start the model evaluation\n",
    "         model.eval()\n",
    "\n",
    "         # Refine the accuracy of the prediction without updating the gradients\n",
    "         with torch.no_grad():\n",
    "\n",
    "            # Load in all of the image inputs and labels from the TEST loader \n",
    "            for inputs, labels in testloader:\n",
    "\n",
    "               # Load the inputs and labels to the already selected device\n",
    "               inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "               # Pass the images through the model, return the log probabilities of each label\n",
    "               logps = model.forward(inputs)\n",
    "\n",
    "               # Run the log probabilities through the criterion to get the output graph\n",
    "               batch_loss = criterion(logps, labels)\n",
    "\n",
    "               # Add the actual loss number to the running loss total for the test batch\n",
    "               test_loss += batch_loss.item()\n",
    "\n",
    "               # Return a new tensor with the true probabilities\n",
    "               ps = torch.exp(logps)\n",
    "\n",
    "               # Return the largest probability and class of the new tensor along a given dimension\n",
    "               top_p, top_class = ps.topk(1, dim=1)\n",
    "\n",
    "               # Reshape the tensor to match the same shape as the top class\n",
    "               equals = top_class == labels.view(*top_class.shape)\n",
    "\n",
    "               # Compute the accuracy and add it to the running accuracy count for the test batch\n",
    "               accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "         # Append the training and testing losses\n",
    "         train_losses.append(running_loss/len(trainloader))\n",
    "         test_losses.append(test_loss/len(testloader))  \n",
    "\n",
    "         # Display the accuracy of the prediction with 3 digits in the fractional part of the decimal\n",
    "         print(f\"\\n     Epoch {epoch}/{epochs}: \"\n",
    "               f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "               f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "               f\"Test accuracy: {accuracy/len(testloader):.3f}\\n\")\n",
    "\n",
    "         # Train the model\n",
    "         running_loss = 0\n",
    "         model.train()\n",
    "\n",
    "         # After 10 training steps, start the next epoch\n",
    "         # Break here in case the trainloader has remaining data\n",
    "         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the training output\n",
    "After five epochs are complete, the system reaches our epoch limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the prediction accuracy for each epoch iteration with training and testing losses, and the test accuracy.\n",
    "\n",
    "Here are the results from our test with five epochs. Your specific results will differ because the computer chooses a set of random images for each test run. The results reveal the training loss, testing loss, and accuracy, all depend on the chosen image.\n",
    "\n",
    "| Epoch\t| Training loss\t| Test loss\t| Test accuracy |\n",
    "| ----- | ------------- | --------- | ------------- |\n",
    "| 1\t    | 0.550\t        | 0.282\t    | 0.902         |\n",
    "| 2\t    | 0.451\t        | 0.311\t    | 0.842         |\n",
    "| 3\t    | 0.342\t        | 0.233\t    | 0.902         |\n",
    "| 4\t    | 0.216\t        | 0.189\t    | 0.906         |\n",
    "| 5\t    | 0.234\t        | 0.175\t    | 0.935         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code calculates and displays the accuracy of our AI model to classify the rock type.\n",
    "print(accuracy/len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model...\n",
    "torch.save(model, 'aerialmodel.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model to make predictions\n",
    "\n",
    "Steps to make predictions with the neural network in your AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load the neural network\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=torch.load('aerialmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the images to test the model\n",
    "test_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                          transforms.Resize(224),\n",
    "                                          transforms.ToTensor(),\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                          transforms.Resize(224),\n",
    "                                          transforms.ToTensor(),\n",
    "                                      ])\n",
    "\n",
    "# Create a function to load random images (n)\n",
    "def get_random_images(data, num):\n",
    "    indices = list(range(len(data)))\n",
    "    np.random.shuffle(indices)\n",
    "    idx = indices[:num]\n",
    "    sampler = SubsetRandomSampler(idx)\n",
    "    loader = torch.utils.data.DataLoader(data, sampler=sampler, batch_size=num)\n",
    "    for images, labels in loader:\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to predict the type of rock in a new image by comparing it against our model's matrix pattern\n",
    "def predict_image(image):\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    index = output.data.cpu().numpy().argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a neural network that classifies photos of space rocks\n",
    "Start by telling the computer to select five images by random for the first test. After the first test, run the code again with more images. \n",
    "1. Use the data from the trainloader variable again, and pass each image to our functions to extract rock features from the photo.\n",
    "2. The computer compares the features to the patterns recognized by the model.\n",
    "3. With that information, the model predicts the type of rock in the photo. \n",
    "4. The last step is to use the plt function to plot a graph of the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get five random images and display them in a figure with their labels\n",
    "to_pil = transforms.ToPILImage()\n",
    "test_data = datasets.ImageFolder(data_dir, transform=test_transforms)\n",
    "images, labels = get_random_images(test_data, 20)\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "\n",
    "# Load all of the classes from the training loader\n",
    "classes=trainloader.dataset.classes\n",
    "\n",
    "# Loop through the 5 randomly selected images\n",
    "for ii in range(len(images)):\n",
    "\n",
    "    # Predict the class of each image\n",
    "    image = to_pil(images[ii])\n",
    "    index = predict_image(image)\n",
    "\n",
    "    # Add the class to the plot graph to display beneath the image\n",
    "    sub = fig.add_subplot(1, len(images), ii+1)\n",
    "    res = int(labels[ii]) == index\n",
    "    sub.set_title(str(classes[index]) + \":\" + str(res))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "\n",
    "# Reshow the plot with the predicted labels beneath the images\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
